---
title: "White paper: Aquatic Joint Research Platform"
output: html_vignette
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```

NOTE - Some ideas on the collection > modelling > prediction/classification/forecast/scenario testing process. 

## The problem

Currently there are a large number of modelling and classification tools available for understanding and responding to environmental harm in aquatic systems. They do not share common input or output formats. Making integration and holistic understanding of the environment challenging: 

 - Tools use different data structures and formats for input and outputs
 - There is no single repository of 'reference' sites and results.
 - There is no single repository for predictive variables (slope, discharge, geology, alkalinity etc)
 - There is no single taxonomic reference table or standard 
 - Confidence and uncertainty in results are calculated and presented in different ways
 - All tools/models are development in isolation
 - There is limited use in scenario planning (for instance how landuse changes or new discharges could affect the environment)
 - There is limited use in climate change or environmental forecasting
 - Best practice and associated tools for auditing and quality control are not shared across methods

## Joint research platform

To aid collaboration and to response to the changing environmentally pressures, we propose creating a joint research platform to share understanding on the environment while lightening the burden of more mundane tasks involved in maintaining and deploying models.

This is influenced by the work of climate change research and weather forecasting communities. As well as larger science projects such as CERN. And smaller scale example such as ropenSci. 

## Who should be included in the reseach platform

Most regulatory modelling work is development under the UKTAG group. This group has a number of sub-groups for freshwater, marine etc. The organisation has worked well and is active. The UKTAG group and it's nominated leads in the devolved agencies would continue to develop methods and tools. However, these tools would be incorporated into a joint platform for delivery. Where tools are agency specific, these could also make use of the platform. 

Currently some models for environmental are shared on github e.g. phybenthos, fsc2, rict. They are published in different areas and have different access controls. The proposal would seek to share these tools under a single repo once the tool is finalised.

As agencies commission new tools to be developed, researchers can 

### How can we collaborate on building models 

> Aren't all methods snowflakes?

Ecological modelling relies on sampling. The samples come in a range of forms from points, transects, images, areas, grabs etc. But the general feature of modelling is based on being able to predict what we expect to find from whatever sampling technique we deploy. The sample is the fundamental observation which we compare against our expectation. The samples are discreet and independent, either observed instantaneously or perhaps over a few minutes or hours (where dynamic changes are not significant).

Multiple samples can be aggregated to smooth variance but the sample still remains the fundamental building block. The sample could be a single pixel from an aerial image or a salmon moving through a fish counter. We still make predictions of what we expect this sample to be like even if the true picture only emerges after several samples are aggregated or compared.

## How does this look?

Below is an example of diatom records, invert data and river flow in a shared input format. 

They share some reference/book keeping variables but not all. Ultimately, only one reference is needed which is a unique sample id. The other reference variables can be 'nested' or in others words there can be as many or a few as you like. These nested values could be sample type, collector, instrument details etc. Or variables later used for aggregation such as water body, river, geographic area etc. For example:

```{r}
data <- utils::read.csv(system.file("extdat",
                                    "test-data.csv",
                                    package = "rict"
))


knitr::kable(data.frame("sample_id" = 192342, "Nested meta data" = names(data[, 2:6]), check.names = FALSE))


```

## Predictors

Predictive variables such as temperature, altitude, slope etc are also metadata can can be nested, as they are the same for each sample. 


## Observations 

The observation comes in two parts, the unique name/id for what you are observing and the value associated with it. For clarity these variables are called 'question' and 'response'. The question could be "Taxon name?" and the response "black bird". However the question must be unique. As 'Taxon name?' will be different depending on what survey is being undertaken. So question_id and response_id are given to allow these to be unique with metadata to provide a more readable format. 

UUID are used to provide unique IDs for this variables. 

```{r}

 data <- tidyr::nest(data, meta_variables = c(2:6, 9:26))
# data <- nest(meta_variables = c(2:6))
knitr::kable(data[1:2,])

```

## Data input

Data input is through mobile or internet connected devices. The question_id and related meta data is configured. Manual or automated data can be collected.

## Model platform

There is no prescribed modelling program or software. Researchers can download the data provided and use any software they desire - as long as it has an api. 

Alternatively, if researchers can't provide an api for others. The recommendation is to use R - which integrates into the data collection back-end. 

Once model is complete in R, the model object is saved and deployed. Any existing or future data collected using the platform will be run through the model at the sample level.

Researchers can then build tools to display and aggregate the sample level results as required. 













